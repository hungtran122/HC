{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from libs.utils import *\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 10000\n",
    "file_path = './checkpoint/keras_best.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, app_cat_cols = application_train_test(num_rows)\n",
    "# with timer(\"Process bureau and bureau_balance\"):\n",
    "#     bureau, b_cat_cols = bureau_and_balance(num_rows)\n",
    "#     print(\"Bureau df shape:\", bureau.shape)\n",
    "#     df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "#     del bureau\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timer(\"Process previous_applications\"):\n",
    "#         prev, prev_cat_cols = previous_applications(num_rows)\n",
    "#         print(\"Previous applications df shape:\", prev.shape)\n",
    "#         df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "#         del prev\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timer(\"Process POS-CASH balance\"):\n",
    "#         pos, pos_cat_cols = pos_cash(num_rows)\n",
    "#         print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "#         df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "#         del pos\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timer(\"Process installments payments\"):\n",
    "#         ins, ins_cat_cols = installments_payments(num_rows)\n",
    "#         print(\"Installments payments df shape:\", ins.shape)\n",
    "#         df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "#         del ins\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with timer(\"Process credit card balance\"):\n",
    "#         cc, cc_cat_cols = credit_card_balance(num_rows)\n",
    "#         print(\"Credit card balance df shape:\", cc.shape)\n",
    "#         df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "#         del cc\n",
    "#         gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_cols = app_cat_cols + b_cat_cols + prev_cat_cols + pos_cat_cols + ins_cat_cols + cc_cat_cols\n",
    "# for c in cat_cols:\n",
    "#     print(c, df[c].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 10000, test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# tr_df, te_df, cat_cols = application_train_test(num_rows)\n",
    "full_df, tr_size, cat_cols = application_train_test(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = full_df[:tr_size].TARGET\n",
    "le = preprocessing.LabelEncoder()\n",
    "allvalues = y.unique().tolist()\n",
    "le.fit(allvalues)\n",
    "y = le.transform(y.values)\n",
    "# y = to_categorical(y)\n",
    "cat_len = []\n",
    "for c in cat_cols:\n",
    "    cat_len.append(len(full_df[c].unique()))\n",
    "#cat_cols = [col for col in df.columns if df[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AMT_INCOME_TOTAL', 'ENTRANCES_MEDI', 'NEW_PHONE_TO_BIRTH_RATIO_EMPLOYER', 'FLOORSMIN_MEDI', 'ELEVATORS_MODE', 'LIVINGAPARTMENTS_MEDI', 'YEARS_BUILD_AVG', 'APARTMENTS_MEDI', 'FLAG_CONT_MOBILE', 'NONLIVINGAREA_MEDI', 'DAYS_LAST_PHONE_CHANGE', 'AMT_REQ_CREDIT_BUREAU_DAY', 'ENTRANCES_AVG', 'COMMONAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'AMT_ANNUITY', 'LANDAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'YEARS_BUILD_MODE', 'DAYS_EMPLOYED', 'NEW_CAR_TO_BIRTH_RATIO', 'AMT_GOODS_PRICE', 'TARGET', 'EXT_SOURCE_3', 'NEW_SCORES_STD', 'FLAG_OWN_REALTY', 'FLAG_DOCUMENT_3', 'APARTMENTS_MODE', 'NEW_CREDIT_TO_ANNUITY_RATIO', 'AMT_REQ_CREDIT_BUREAU_QRT', 'COMMONAREA_AVG', 'YEARS_BEGINEXPLUATATION_MEDI', 'NEW_INC_BY_ORG', 'BASEMENTAREA_MEDI', 'NEW_INC_PER_CHLD', 'FLAG_OWN_CAR', 'HOUR_APPR_PROCESS_START', 'LIVINGAREA_MEDI', 'ELEVATORS_MEDI', 'LIVINGAPARTMENTS_AVG', 'ENTRANCES_MODE', 'NEW_DOC_IND_KURT', 'DEF_30_CNT_SOCIAL_CIRCLE', 'NEW_EXT_SOURCES_MEAN', 'LIVINGAREA_AVG', 'DAYS_REGISTRATION', 'REG_CITY_NOT_LIVE_CITY', 'LIVE_REGION_NOT_WORK_REGION', 'BASEMENTAREA_AVG', 'AMT_REQ_CREDIT_BUREAU_YEAR', 'YEARS_BUILD_MEDI', 'LIVINGAREA_MODE', 'CNT_FAM_MEMBERS', 'AMT_CREDIT', 'LIVINGAPARTMENTS_MODE', 'REG_CITY_NOT_WORK_CITY', 'FLAG_PHONE', 'NEW_CREDIT_TO_GOODS_RATIO', 'APARTMENTS_AVG', 'CODE_GENDER', 'EXT_SOURCE_2', 'NEW_ANNUITY_TO_INCOME_RATIO', 'OBS_60_CNT_SOCIAL_CIRCLE', 'FLAG_WORK_PHONE', 'DAYS_BIRTH', 'LANDAREA_MEDI', 'FLAG_MOBIL', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'CNT_CHILDREN', 'NONLIVINGAPARTMENTS_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'REG_REGION_NOT_WORK_REGION', 'EXT_SOURCE_1', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'ELEVATORS_AVG', 'NEW_SOURCES_PROD', 'DEF_60_CNT_SOCIAL_CIRCLE', 'NEW_PHONE_TO_BIRTH_RATIO', 'FLOORSMAX_MEDI', 'FLOORSMAX_AVG', 'OBS_30_CNT_SOCIAL_CIRCLE', 'OWN_CAR_AGE', 'REGION_POPULATION_RELATIVE', 'DAYS_ID_PUBLISH', 'FLOORSMIN_MODE', 'REG_REGION_NOT_LIVE_REGION', 'LIVE_CITY_NOT_WORK_CITY', 'NEW_LIVE_IND_SUM', 'FLAG_EMP_PHONE', 'NEW_EMPLOY_TO_BIRTH_RATIO', 'COMMONAREA_MODE', 'FLOORSMIN_AVG', 'SK_ID_CURR', 'NONLIVINGAREA_AVG', 'REGION_RATING_CLIENT_W_CITY', 'REGION_RATING_CLIENT', 'NEW_CREDIT_TO_INCOME_RATIO', 'NONLIVINGAREA_MODE', 'YEARS_BEGINEXPLUATATION_AVG', 'FLAG_EMAIL', 'NEW_CAR_TO_EMPLOY_RATIO', 'BASEMENTAREA_MODE', 'FLOORSMAX_MODE', 'LANDAREA_MODE', 'TOTALAREA_MODE']\n"
     ]
    }
   ],
   "source": [
    "num_cols = list(set(full_df.columns.tolist()) - set(cat_cols + ['index']))\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "import tensorflow as tf\n",
    "def nn_model(X_num, X_cat , cat_len = cat_len, cat_cols = cat_cols, num_cols = num_cols):\n",
    "#     cat_out = []\n",
    "#     for c in cat_cols:\n",
    "#         cat_input = df[c].copy()\n",
    "#         in_dim = len(df[c].unique())\n",
    "#         out_dim = in_dim // 2\n",
    "#         embed = Embedding(input_dim=in_dim, output_dim = out_dim)\n",
    "#         out = embed(cat_input)\n",
    "    input_cat = []\n",
    "    out_cat = []\n",
    "    print(X_cat.shape[1])\n",
    "    for name in cat_cols:\n",
    "        input_cat.append(Input(shape=(1,), name=\"cat_\" + name))\n",
    "    for x, c_len in zip(input_cat, cat_len):\n",
    "        x = Embedding(c_len + 1, c_len // 2, embeddings_initializer =\"normal\")(x)\n",
    "        #x = SpatialDropout1D(0.25)(x)\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(c_len * 8, activation=\"relu\", kernel_initializer=\"normal\")(x)\n",
    "        out_cat.append(x)\n",
    "    out_cat = concatenate(out_cat, axis=-1)\n",
    "    out_cat = Dense(64, activation=\"relu\", kernel_initializer=\"normal\")(out_cat)\n",
    "    \n",
    "    input_num = Input(shape=(X_num.shape[1],), name=\"numeric\")\n",
    "    out_num = Dense(64, activation=\"relu\", kernel_initializer=\"normal\")(input_num)\n",
    "    out_num = BatchNormalization()(out_num)\n",
    "    out_num = Dropout(0.5)(out_num)\n",
    "    \n",
    "    out_num_cat = concatenate([out_cat, out_num], axis=-1)\n",
    "    print(out_num_cat.shape)\n",
    "    out_num_cat = Dense(64, activation=\"relu\", kernel_initializer=\"normal\")(out_num_cat)\n",
    "    out_num_cat = BatchNormalization()(out_num_cat)\n",
    "    print(out_num_cat.shape)\n",
    "    out_num_cat = Dense(1, activation=\"sigmoid\", kernel_initializer=\"normal\")(out_num_cat)\n",
    "    print(out_num_cat.shape)\n",
    "    for i in input_cat:\n",
    "        print(i)\n",
    "    model = Model(inputs=[*input_cat, input_num], outputs=out_num_cat)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=2, save_best_only=True, save_weights_only=True,\n",
    "                                 mode='min')\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, min_delta=1e-4)\n",
    "lr_reduced = ReduceLROnPlateau(monitor='val_loss',\n",
    "                               factor=0.5,\n",
    "                               patience=3,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "WARNING:tensorflow:From C:\\Users\\hungtran\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(?, 128)\n",
      "(?, 64)\n",
      "(?, 1)\n",
      "Tensor(\"cat_EMERGENCYSTATE_MODE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_FONDKAPREMONT_MODE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_HOUSETYPE_MODE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_NAME_CONTRACT_TYPE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_NAME_EDUCATION_TYPE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_NAME_FAMILY_STATUS:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_NAME_HOUSING_TYPE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_NAME_INCOME_TYPE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_NAME_TYPE_SUITE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_OCCUPATION_TYPE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_ORGANIZATION_TYPE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_WALLSMATERIAL_MODE:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_WEEKDAY_APPR_PROCESS_START:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_FLAG_DOCUMENT_5:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_FLAG_DOCUMENT_6:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_FLAG_DOCUMENT_8:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_FLAG_DOCUMENT_14:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_FLAG_DOCUMENT_16:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"cat_FLAG_DOCUMENT_18:0\", shape=(?, 1), dtype=float32)\n",
      "WARNING:tensorflow:From C:\\Users\\hungtran\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "te_df = full_df[tr_size:]\n",
    "tr_df = full_df[:tr_size]\n",
    "X_num = np.array(tr_df[num_cols])\n",
    "X_cat = np.array(tr_df[cat_cols])\n",
    "model = nn_model(X_num, X_cat)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[+] Fold 0\n",
      "(8000, 106)\n",
      "19\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "7808/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00001: val_loss did not improve\n",
      "8000/8000 [==============================] - 4s 518us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      " 384/8000 [>.............................] - ETA: 2s - loss: nan - acc: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hungtran\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\callbacks.py:405: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current, self.best):\n",
      "C:\\Users\\hungtran\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\callbacks.py:497: RuntimeWarning: invalid value encountered in less\n",
      "  if self.monitor_op(current - self.min_delta, self.best):\n",
      "C:\\Users\\hungtran\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\callbacks.py:870: RuntimeWarning: invalid value encountered in less\n",
      "  self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7872/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00002: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 271us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "7936/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00003: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 271us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "7904/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 0.0005000000237487257.\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "7856/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00005: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "\n",
      "[+] Fold 1\n",
      "(8000, 106)\n",
      "19\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "7888/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00001: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "7904/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00002: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "7808/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00003: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 268us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "7808/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 0.0002500000118743628.\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "7936/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00005: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "\n",
      "[+] Fold 2\n",
      "(8000, 106)\n",
      "19\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "7856/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00001: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "7936/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00002: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "7920/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00003: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "7808/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 0.0001250000059371814.\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "7984/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00005: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "\n",
      "[+] Fold 3\n",
      "(8000, 106)\n",
      "19\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "7984/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00001: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "7968/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00002: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 291us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "7872/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00003: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 302us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "7904/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 6.25000029685907e-05.\n",
      "8000/8000 [==============================] - 2s 271us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "7936/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00005: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "\n",
      "[+] Fold 4\n",
      "(8000, 106)\n",
      "19\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "7808/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00001: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "7888/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00002: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "7856/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00003: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "7856/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00004: val_loss did not improve\n",
      "\n",
      "Epoch 00004: reducing learning rate to 3.125000148429535e-05.\n",
      "8000/8000 [==============================] - 2s 270us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "7856/8000 [============================>.] - ETA: 0s - loss: nan - acc: 0.0000e+00Epoch 00005: val_loss did not improve\n",
      "8000/8000 [==============================] - 2s 272us/step - loss: nan - acc: 0.0000e+00 - val_loss: nan - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model_name = 'keras'\n",
    "checkpoint_path = './checkpoint'\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kfold.split(y)):\n",
    "\n",
    "#     model.summary()\n",
    "    print(f\"\\n[+] Fold {fold}\")\n",
    "\n",
    "    file_path = f\"./checkpoint/keras_{model_name}_best_{fold}.h5\"\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=2, save_best_only=True,\n",
    "                                 save_weights_only=True,\n",
    "                                 mode='min')\n",
    "    csv_logger = CSVLogger(f'{checkpoint_path}/log_{model_name}_{fold}.csv', append=True, separator=',')\n",
    "    callbacks_list = [checkpoint, early, lr_reduced, csv_logger]\n",
    "\n",
    "    X_tr_num =np.array(tr_df[num_cols])[train_index]\n",
    "    X_tr_cat = [tr_df[c][train_index] for c in cat_cols]\n",
    "    y_tr = y[train_index]\n",
    "    \n",
    "    print(X_tr_num.shape)\n",
    "    print(len(X_tr_cat))\n",
    "\n",
    "    X_val_num = np.array(tr_df[num_cols])[val_index]\n",
    "    X_val_cat = [tr_df[c][val_index] for c in cat_cols]\n",
    "    y_val = y[val_index]\n",
    "\n",
    "    history = model.fit([*X_tr_cat, X_tr_num ], y_tr,\n",
    "                        validation_data=([*X_val_cat, X_val_num], y_val),\n",
    "                        verbose=1, callbacks=callbacks_list,\n",
    "                        epochs=10, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([*X_tr_cat, X_tr_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
